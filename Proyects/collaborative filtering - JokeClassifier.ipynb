{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5f5b271",
   "metadata": {},
   "source": [
    "# Collaborative Filtering\n",
    "\n",
    "Busco modelar la calificacion del usuario j al item i, dado un conjunto de clasificaciones de otros usuarios.\n",
    "\n",
    "> $y(i,j) = w^j*x^i+b^j$ Es la calificacion del user j al item i.\n",
    "\n",
    "Entonces \n",
    "- $X^i$: vector del item\n",
    "- $W^j$, $B^j$: vectores del usuario\n",
    "\n",
    "Obs. $W^j$, $X^i$ tienen misma dimension.\n",
    "\n",
    "Por lo tanto, busco $W$, $X$, $B$ para minimizar el error J.\n",
    "\n",
    "En particular, cuando la calificacion no es binaria, el error es muy parecido a varios modelos de aprendizaje supervizado no binarios:\n",
    "\n",
    "> $J(W,B,X) = 1/2\\sum_{(i,j):r(i,j)=1}(W^j*X^i+B^j-Y^{(i,j)})^2 + \\lambda/2(\\sum^{nU}_{j}\\sum_{k}^{n}(w_k^j)^2 + \\sum^{nI}_{i}\\sum_{k}^{n}(x_k^i)^2)$\n",
    "\n",
    "Siendo $r(i,j) | r(i,j) = 1 <=> $ el item i fue clasificado por el usuario j. \n",
    "\n",
    "Cuando la clasificacion es binaria (like, follow, click, etc), en cambio es: \n",
    "\n",
    "> $J(W,B,X) = \\sum_{(i,j):r(i,j)=1}L(f^{*}_{w,b,x}(x)-Y^{(i,j)})$\n",
    "\n",
    "> $L(f^*_{w,b,x}(x),y^{i,j}) = -y^{(i,j)}*log(f^{*}(x))-(1-y^{(i,j)})*log(1-f^{*}(x))$\n",
    "\n",
    "> $f^{*}_{w,b,x}(x) = g(W^j*X^i+B^j)$\n",
    "\n",
    "> $g(z) = \\frac{1}{1 + e^{-z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ca7bca",
   "metadata": {},
   "source": [
    "### Implementacion ejemplo\n",
    "\n",
    "Voy a hacer una implementacion para recomendar items en base a calificaciones (de -10 a +10) que hicieron usuarios sobre distintos chistes. \n",
    "\n",
    "- Kaggle Dataset: https://www.kaggle.com/datasets/aakaashjois/jester-collaborative-filtering-dataset?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3bf5aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PATH_TO_DATASET = r\"joke-rating\\UserRatings1.csv\"\n",
    "df = pd.read_csv(PATH_TO_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "04f897d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JokeId</th>\n",
       "      <th>User1</th>\n",
       "      <th>User2</th>\n",
       "      <th>User3</th>\n",
       "      <th>User4</th>\n",
       "      <th>User5</th>\n",
       "      <th>User6</th>\n",
       "      <th>User7</th>\n",
       "      <th>User8</th>\n",
       "      <th>User9</th>\n",
       "      <th>...</th>\n",
       "      <th>User36701</th>\n",
       "      <th>User36702</th>\n",
       "      <th>User36703</th>\n",
       "      <th>User36704</th>\n",
       "      <th>User36705</th>\n",
       "      <th>User36706</th>\n",
       "      <th>User36707</th>\n",
       "      <th>User36708</th>\n",
       "      <th>User36709</th>\n",
       "      <th>User36710</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.10</td>\n",
       "      <td>-8.79</td>\n",
       "      <td>-3.50</td>\n",
       "      <td>7.14</td>\n",
       "      <td>-8.79</td>\n",
       "      <td>9.22</td>\n",
       "      <td>-4.03</td>\n",
       "      <td>3.11</td>\n",
       "      <td>-3.64</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.91</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.90</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>-2.91</td>\n",
       "      <td>-3.88</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>9.37</td>\n",
       "      <td>-1.55</td>\n",
       "      <td>0.92</td>\n",
       "      <td>-3.35</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-6.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.60</td>\n",
       "      <td>-4.56</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>-3.06</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>-3.93</td>\n",
       "      <td>-3.64</td>\n",
       "      <td>7.52</td>\n",
       "      <td>-6.46</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.98</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-4.17</td>\n",
       "      <td>-4.61</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>8.98</td>\n",
       "      <td>9.27</td>\n",
       "      <td>-6.99</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-3.40</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.15</td>\n",
       "      <td>5.39</td>\n",
       "      <td>7.52</td>\n",
       "      <td>6.26</td>\n",
       "      <td>7.67</td>\n",
       "      <td>3.45</td>\n",
       "      <td>5.44</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>1.26</td>\n",
       "      <td>...</td>\n",
       "      <td>2.28</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>5.1</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-3.54</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>7.48</td>\n",
       "      <td>-5.78</td>\n",
       "      <td>0.73</td>\n",
       "      <td>2.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36711 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   JokeId  User1  User2  User3  User4  User5  User6  User7  User8  User9  ...  \\\n",
       "0       0   5.10  -8.79  -3.50   7.14  -8.79   9.22  -4.03   3.11  -3.64  ...   \n",
       "1       1   4.90  -0.87  -2.91  -3.88  -0.58   9.37  -1.55   0.92  -3.35  ...   \n",
       "2       2   1.75   1.99  -2.18  -3.06  -0.58  -3.93  -3.64   7.52  -6.46  ...   \n",
       "3       3  -4.17  -4.61  -0.10   0.05   8.98   9.27  -6.99   0.49  -3.40  ...   \n",
       "4       4   5.15   5.39   7.52   6.26   7.67   3.45   5.44  -0.58   1.26  ...   \n",
       "\n",
       "   User36701  User36702  User36703  User36704  User36705  User36706  \\\n",
       "0        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "1        NaN        NaN        NaN      -5.63        NaN      -6.07   \n",
       "2        NaN        NaN        NaN        NaN        NaN       4.08   \n",
       "3        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "4       2.28      -0.49        5.1      -0.29      -3.54      -1.36   \n",
       "\n",
       "   User36707  User36708  User36709  User36710  \n",
       "0        NaN        NaN       2.91        NaN  \n",
       "1        NaN      -1.60      -4.56        NaN  \n",
       "2        NaN        NaN       8.98        NaN  \n",
       "3        NaN        NaN        NaN        NaN  \n",
       "4       7.48      -5.78       0.73       2.62  \n",
       "\n",
       "[5 rows x 36711 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6a84c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = 100\n",
    "num_jokes = 100\n",
    "num_features = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716de9c",
   "metadata": {},
   "source": [
    "La idea es poder recomendar chistes similares a las personas segun sus gustos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b4b3effd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 100), (100, 100))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializo las variables W, B, X con valores aleatorios y a R segun si el usuario califico o no el chiste.\n",
    "# Tambien inicializo a Y\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "W = tf.Variable(tf.random.normal((num_users, num_features), dtype=tf.float64),  name='W')\n",
    "X = tf.Variable(tf.random.normal((num_jokes, num_features), dtype=tf.float64),  name='X')\n",
    "b = tf.Variable(tf.random.normal((1,          num_users),   dtype=tf.float64),  name='b')\n",
    "\n",
    "Y = df.to_numpy() # de esta forma solo devuelve 100 filas\n",
    "Y = Y [0:num_jokes, 1:num_users+1]\n",
    "\n",
    "R = np.zeros([num_jokes, num_users])\n",
    "\n",
    "for j in range(Y.shape[1]):\n",
    "    for i in range(Y.shape[0]):\n",
    "        R[j,i] = int (Y[j,i] is not np.nan) \n",
    "\n",
    "R.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9c1ee4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalizo a Y\n",
    "Ynorm = Y / np.linalg.norm(Y, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67796eff",
   "metadata": {},
   "source": [
    "### Defino la funcion error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5d9aad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_ERROR (W, B, X, Y, R, lambda_=0.1):\n",
    "    j = (tf.linalg.matmul(X, tf.transpose(W)) + b - Y)*R\n",
    "    J = 0.5 * tf.reduce_sum(j**2) + (lambda_/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6840db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancio un optimizador\n",
    "lambda_ = 1e-1\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7cb157ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ #0 Loss: 0.15501157403958468\n",
      "+ #100 Loss: 0.1545140066790011\n",
      "+ #200 Loss: 0.15419706048975237\n",
      "+ #300 Loss: 0.15399174690512168\n",
      "+ #400 Loss: 0.15385646539706097\n",
      "+ #500 Loss: 0.15376588744369052\n",
      "+ #600 Loss: 0.15370437566259484\n",
      "+ #700 Loss: 0.15385297908258783\n",
      "+ #800 Loss: 0.15363273990525242\n",
      "+ #900 Loss: 0.15414246824539324\n"
     ]
    }
   ],
   "source": [
    "iterations = 1000\n",
    "for iter in range(iterations):\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost_value = J_ERROR (W, b, X, Ynorm, R, lambda_)\n",
    "    grads = tape.gradient( cost_value, [X,W,b] )\n",
    "    optimizer.apply_gradients( zip(grads, [X,W,b]) )\n",
    "    if iter % 100 == 0:\n",
    "        print (f\"+ #{iter} Loss: {cost_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d3213d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.06819757e+00,  3.59843867e+00, -1.43783275e+00, ...,\n",
       "        -8.54084165e-01,  2.00316317e-01,  7.08969457e-01],\n",
       "       [-9.70526431e-02, -7.72529700e-01,  3.49686450e+00, ...,\n",
       "         8.55165902e+00, -1.39149536e+00, -5.94163142e+00],\n",
       "       [-2.07225659e+00, -1.76538034e+00, -2.76461024e-01, ...,\n",
       "        -3.71469927e+00, -6.95666572e-01, -3.01465960e+00],\n",
       "       ...,\n",
       "       [-2.28643680e-01,  3.00586026e+00, -2.86884339e+00, ...,\n",
       "        -3.62887059e+00,  1.70803215e+00, -6.85146832e-01],\n",
       "       [ 3.19796524e+00,  1.17722896e+00, -1.49379193e+00, ...,\n",
       "         2.16716152e+00, -9.94588596e-01,  1.37811044e+00],\n",
       "       [-6.65866373e+00, -3.11564238e+00,  6.25193232e-03, ...,\n",
       "         4.55544559e+00, -1.37444689e+00,  3.57848081e-01]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Armo la matriz de predicciones\n",
    "preds = np.matmul(X.numpy(), np.transpose(W.numpy())) + b.numpy()\n",
    "preds += Ynorm\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "bc5fe0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ User:  -1.26\n",
      "+ Model:  -2.9\n",
      "+ Error:  8.2 %\n"
     ]
    }
   ],
   "source": [
    "def compare_model_rating (joke_i, user_j):\n",
    "    if R[joke_i, user_j] == 0:\n",
    "        print (\"+ El usuario no habia clasificado el chiste. \")\n",
    "        print (\"+ El modelo predice un rating de \", round(preds[joke_i,user_j], 2))\n",
    "    else: \n",
    "        print (\"+ User: \", Y[joke_i,user_j])\n",
    "        print (\"+ Model: \", round(preds[joke_i,user_j], 2))\n",
    "        print (\"+ Error: \", round(100*abs((Y[joke_i,user_j] - preds[joke_i,user_j])/20), 1), \"%\")\n",
    "\n",
    "compare_model_rating(17, 22)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b7b12a9",
   "metadata": {},
   "source": [
    "### Armo una clase que haga todo lo que esta codificado de forma organizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7effd975",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class JokeRecommender ():\n",
    "\n",
    "    def __init__ (self, num_users, num_jokes):\n",
    "\n",
    "        # Inicio variables del modelo\n",
    "        self.W = tf.Variable(tf.random.normal((num_users, num_features), dtype=tf.float64),  name='W')\n",
    "        self.X = tf.Variable(tf.random.normal((num_jokes, num_features), dtype=tf.float64),  name='X')\n",
    "        self.b = tf.Variable(tf.random.normal((1,          num_users),   dtype=tf.float64),  name='b')\n",
    "\n",
    "        # Leo Y\n",
    "        Y = df.to_numpy() # devuelve 100 filas solo\n",
    "        self.Y = Y [0:num_jokes, 1:num_users+1]\n",
    "\n",
    "        # Armo matriz R\n",
    "        self.R = np.zeros([num_jokes, num_users])\n",
    "        for j in range(self.Y.shape[1]):\n",
    "            for i in range(self.Y.shape[0]):\n",
    "                self.R[j,i] = int (Y[j,i] is not np.nan) \n",
    "\n",
    "        ## Creo Y normalizado\n",
    "        self.Ymean = np.linalg.norm(Y, 1)\n",
    "        self.Ynorm = self.Y / self.Ymean\n",
    "\n",
    "        # Creo la matriz que va a guardar las predicciones del modelo entrenado\n",
    "        self.preds = np.zeros([num_jokes, num_users])\n",
    "        self.lambda_ = 1e-1\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=lambda_)\n",
    "\n",
    "\n",
    "    def fit (self, _iterations):\n",
    "        iterations = _iterations\n",
    "        for iter in range(iterations):\n",
    "            with tf.GradientTape() as tape:\n",
    "                cost_value = J_ERROR (self.W, self.b, self.X, self.Ynorm, self.R, self.lambda_)\n",
    "            grads = tape.gradient( cost_value, [self.X, self.W, self.b] )\n",
    "            self.optimizer.apply_gradients( zip(grads, [self.X, self.W, self.b]) )\n",
    "            if iter % 100 == 0:\n",
    "                print (f\"+ #{iter} Loss: {cost_value}\")\n",
    "\n",
    "        # Actualizo la matriz de predicciones\n",
    "        self.preds = np.matmul(X.numpy(), np.transpose(W.numpy())) + b.numpy()\n",
    "\n",
    "    def predict (self, joke_i, user_j):\n",
    "        return self.preds[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "051f2b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "+ #0 Loss: nan\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['b:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 7.06180995,  3.60944792, -1.43344909, ..., -0.85427204,\n",
       "         0.19454241,  0.70045264],\n",
       "       [-0.10318976, -0.77144005,  3.5005092 , ...,  8.55147115,\n",
       "        -1.39818357, -5.9470421 ],\n",
       "       [-2.07444842, -1.76787276, -0.27373063, ..., -3.71531299,\n",
       "        -0.70380765, -3.02548098],\n",
       "       ...,\n",
       "       [-0.22840571,  3.00068755, -2.86021385, ..., -3.62850737,\n",
       "         1.713017  , -0.67705585],\n",
       "       [ 3.1938947 ,  1.17953351, -1.49087367, ...,  2.16053594,\n",
       "        -0.99361167,  1.37069579],\n",
       "       [-6.66413704, -3.1193497 ,  0.01549519, ...,  4.5648141 ,\n",
       "        -1.36587997,  0.34811636]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrecomm = JokeRecommender(100, 100)\n",
    "myrecomm.fit(100)\n",
    "myrecomm.preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "6085004884d7fe2d487ef9b1c33a506ee19a6a4095578a744a51e1ad1ca70fa0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
